{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9494980,"sourceType":"datasetVersion","datasetId":5777635},{"sourceId":9502524,"sourceType":"datasetVersion","datasetId":5783277},{"sourceId":9502752,"sourceType":"datasetVersion","datasetId":5783427},{"sourceId":9503033,"sourceType":"datasetVersion","datasetId":5783636},{"sourceId":9505961,"sourceType":"datasetVersion","datasetId":5785736}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom catboost import CatBoostClassifier, CatBoostRegressor","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:39.323308Z","iopub.execute_input":"2024-09-29T06:28:39.324258Z","iopub.status.idle":"2024-09-29T06:28:39.329573Z","shell.execute_reply.started":"2024-09-29T06:28:39.324209Z","shell.execute_reply":"2024-09-29T06:28:39.328576Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/rutube-dataset/' # Ваш путь до директории с данными /path/to/data/\ndata = pd.read_csv(path + 'train_events.csv')\nvideo = pd.read_csv(path + 'video_info_v2.csv')\ntargets = pd.read_csv(path + 'train_targets.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:39.331507Z","iopub.execute_input":"2024-09-29T06:28:39.332183Z","iopub.status.idle":"2024-09-29T06:28:45.384184Z","shell.execute_reply.started":"2024-09-29T06:28:39.332108Z","shell.execute_reply":"2024-09-29T06:28:45.383329Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"path_video = '/kaggle/input/video-info-sentence/video_info_sentence_trans.parquet'\nvideo2 = pd.read_parquet(path_video)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:45.386301Z","iopub.execute_input":"2024-09-29T06:28:45.386713Z","iopub.status.idle":"2024-09-29T06:28:47.453710Z","shell.execute_reply.started":"2024-09-29T06:28:45.386666Z","shell.execute_reply":"2024-09-29T06:28:47.452603Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"path_video = '/kaggle/input/video-sentence-max/video_embed_cols.csv'\nvideo_m = pd.read_csv(path_video)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:47.454932Z","iopub.execute_input":"2024-09-29T06:28:47.455301Z","iopub.status.idle":"2024-09-29T06:28:53.470522Z","shell.execute_reply.started":"2024-09-29T06:28:47.455265Z","shell.execute_reply":"2024-09-29T06:28:53.469621Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"video_m = video_m.drop(video_m.columns[0], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:53.472606Z","iopub.execute_input":"2024-09-29T06:28:53.472937Z","iopub.status.idle":"2024-09-29T06:28:53.556403Z","shell.execute_reply.started":"2024-09-29T06:28:53.472902Z","shell.execute_reply":"2024-09-29T06:28:53.555569Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"columns = video_m.columns.tolist()\n\n# Переименовываем первые 256 столбцов\nnew_columns = [f'embedding_{i}' if i < 256 else col for i, col in enumerate(columns)]\n\n# Применяем новые имена к DataFrame\nvideo_m.columns = new_columns","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:53.557572Z","iopub.execute_input":"2024-09-29T06:28:53.557877Z","iopub.status.idle":"2024-09-29T06:28:53.563623Z","shell.execute_reply.started":"2024-09-29T06:28:53.557843Z","shell.execute_reply":"2024-09-29T06:28:53.562686Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"def reduce_embeddings_with_pca(df, n_components=50):\n    \"\"\"\n    Уменьшает размерность эмбеддингов в DataFrame с помощью PCA.\n    \n    :param df: DataFrame, содержащий эмбеддинги.\n    :param n_components: Количество компонент для уменьшенной размерности.\n    :return: DataFrame с уменьшенными эмбеддингами.\n    \"\"\"\n    # Выделяем колонки с эмбеддингами\n    embedding_columns = [col for col in df.columns if col.startswith('embedding_')]\n    \n    # Извлекаем эмбеддинги\n    embeddings = df[embedding_columns].values\n    \n    # Применяем PCA\n    pca = PCA(n_components=n_components)\n    reduced_embeddings = pca.fit_transform(embeddings)\n    \n    # Создаем новый DataFrame с уменьшенными эмбеддингами\n    reduced_df = pd.DataFrame(reduced_embeddings, columns=[f'reduced_embedding_{i}' for i in range(n_components)])\n    \n    # Объединяем оригинальный DataFrame с уменьшенными эмбеддингами\n    df_with_reduced_embeddings = pd.concat([df.drop(columns=embedding_columns), reduced_df], axis=1)\n    \n    return df_with_reduced_embeddings","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:53.564831Z","iopub.execute_input":"2024-09-29T06:28:53.565122Z","iopub.status.idle":"2024-09-29T06:28:53.577832Z","shell.execute_reply.started":"2024-09-29T06:28:53.565090Z","shell.execute_reply":"2024-09-29T06:28:53.576709Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"region_time_offsets = {\n    'Adygeya Republic': 3,\n    'Altai': 7,\n    'Altay Kray': 7,\n    'Amur Oblast': 9,\n    'Arkhangelsk Oblast': 3,\n    'Arkhangelskaya': 3,\n    'Astrakhan': 4,\n    'Astrakhan Oblast': 4,\n    'Bashkortostan Republic': 5,\n    'Belgorod Oblast': 3,\n    'Bryansk Oblast': 3,\n    'Buryatiya Republic': 8,\n    'Chechnya': 3,\n    'Chelyabinsk': 5,\n    'Chukotka': 12,\n    'Chuvashia': 3,\n    'Crimea': 3,\n    'Dagestan': 3,\n    'Ingushetiya Republic': 3,\n    'Irkutsk Oblast': 8,\n    'Ivanovo': 3,\n    'Ivanovo Oblast': 3,\n    'Jaroslavl': 3,\n    'Jewish Autonomous Oblast': 10,\n    'Kabardino-Balkariya Republic': 3,\n    'Kaliningrad': 2,\n    'Kaliningrad Oblast': 2,\n    'Kalmykiya Republic': 4,\n    'Kaluga': 3,\n    'Kaluga Oblast': 3,\n    'Kamchatka': 12,\n    'Karachayevo-Cherkesiya Republic': 3,\n    'Karelia': 3,\n    'Kemerovo Oblast': 7,\n    'Khabarovsk': 10,\n    'Khakasiya Republic': 7,\n    'Khanty-Mansia': 5,\n    'Kirov': 3,\n    'Kirov Oblast': 3,\n    'Komi': 3,\n    'Kostroma Oblast': 3,\n    'Krasnodar Krai': 3,\n    'Krasnodarskiy': 3,\n    'Krasnoyarsk Krai': 7,\n    'Krasnoyarskiy': 7,\n    'Kurgan Oblast': 5,\n    'Kursk': 3,\n    'Kursk Oblast': 3,\n    'Kuzbass': 7,\n    \"Leningradskaya Oblast'\": 3,\n    'Lipetsk Oblast': 3,\n    'Magadan Oblast': 11,\n    'Mariy-El Republic': 3,\n    'Mordoviya Republic': 3,\n    'Moscow': 3,\n    'Moscow Oblast': 3,\n    'Murmansk': 3,\n    'Nenets': 3,\n    'Nizhny Novgorod Oblast': 3,\n    'North Ossetia': 3,\n    'North Ossetia–Alania': 3,\n    'Novgorod Oblast': 3,\n    'Novosibirsk Oblast': 7,\n    'Omsk': 6,\n    'Omsk Oblast': 6,\n    'Orel Oblast': 3,\n    'Orenburg Oblast': 5,\n    'Oryol oblast': 3,\n    'Penza': 3,\n    'Penza Oblast': 3,\n    'Perm': 5,\n    'Perm Krai': 5,\n    'Primorskiy (Maritime) Kray': 10,\n    'Primorye': 10,\n    'Pskov Oblast': 3,\n    'Rostov': 3,\n    'Ryazan Oblast': 3,\n    'Sakha': 9,\n    'Sakhalin Oblast': 11,\n    'Samara Oblast': 4,\n    'Saratov Oblast': 4,\n    'Saratovskaya Oblast': 4,\n    'Sebastopol City': 3,\n    'Smolensk': 3,\n    'Smolensk Oblast': 3,\n    'Smolenskaya Oblast’': 3,\n    'St.-Petersburg': 3,\n    'Stavropol Krai': 3,\n    'Stavropol Kray': 3,\n    'Stavropol’ Kray': 3,\n    'Sverdlovsk': 5,\n    'Sverdlovsk Oblast': 5,\n    'Tambov': 3,\n    'Tambov Oblast': 3,\n    'Tatarstan Republic': 3,\n    'Tomsk Oblast': 7,\n    'Transbaikal Territory': 9,\n    'Tula': 3,\n    'Tula Oblast': 3,\n    'Tver Oblast': 3,\n    'Tver’ Oblast': 3,\n    'Tyumen Oblast': 5,\n    'Tyumen’ Oblast': 5,\n    'Tyva Republic': 7,\n    'Udmurtiya Republic': 4,\n    'Ulyanovsk': 4,\n    'Vladimir': 3,\n    'Vladimir Oblast': 3,\n    'Volgograd Oblast': 4,\n    'Vologda': 3,\n    'Vologda Oblast': 3,\n    'Voronezh Oblast': 3,\n    'Voronezj': 3,\n    'Yamalo-Nenets': 5,\n    'Yaroslavl Oblast': 3,\n    'Zabaykalskiy (Transbaikal) Kray': 9\n}\n\ndef calculate_real_timestamp(df):\n    # Получаем смещение для каждого региона\n    df['region_offset'] = df['region'].map(region_time_offsets).fillna(3)  # По умолчанию UTC+3 (Москва)\n    \n    # Преобразуем event_timestamp в формат datetime (если еще не сделано)\n    df['event_timestamp'] = pd.to_datetime(df['event_timestamp'])\n    \n    # Вычисляем реальное время, добавляя разницу между UTC+3 и временем региона\n    df['real_timestamp'] = df['event_timestamp'] + pd.to_timedelta(df['region_offset'] - 3, unit='h')\n        \n    df = df.drop('region_offset', axis=1)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:53.579339Z","iopub.execute_input":"2024-09-29T06:28:53.579642Z","iopub.status.idle":"2024-09-29T06:28:53.597365Z","shell.execute_reply.started":"2024-09-29T06:28:53.579611Z","shell.execute_reply":"2024-09-29T06:28:53.596532Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"class CategoryReducer:\n    def __init__(self, config):\n        self.config = config\n        self.cat_values = {}\n\n    def fit(self, data):\n        \"\"\"\n        Определяет топовые значения для каждой категории на основе конфигурации.\n        \"\"\"\n        top_config = self.config['top_config']\n        for column, take in top_config.items():\n            # Берем топовые значения для каждого столбца\n            top_values = data[column].value_counts().nlargest(take).index.to_list()\n            self.cat_values[column] = top_values\n\n    def transform(self, data):\n        \"\"\"\n        Заменяет редкие значения категорий на 'another_value'.\n        \"\"\"\n        data_copy = data.copy()\n        for column, top_values in self.cat_values.items():\n            data_copy[column] = data_copy[column].apply(lambda x: x if x in top_values else 'another_value')\n        return data_copy","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:53.598517Z","iopub.execute_input":"2024-09-29T06:28:53.598870Z","iopub.status.idle":"2024-09-29T06:28:53.613941Z","shell.execute_reply.started":"2024-09-29T06:28:53.598834Z","shell.execute_reply":"2024-09-29T06:28:53.613085Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"class TargetEncoder:\n    def __init__(self, config):\n        feature_list = config['target_enc']\n        self.feature_list = feature_list\n        self.feature_encoding = {}  # Для хранения среднего возраста и вероятности 'male' для каждой фичи\n    \n    def fit(self, feature_df, target_df):\n        # Объединяем feature_df и target_df по viewer_uid\n        merged_df = feature_df.merge(target_df, on='viewer_uid', how='left')\n        \n        # Для каждой фичи из списка сохраняем средний возраст и вероятность 'male'\n        for feature in self.feature_list:\n            encoding = {}\n            # Считаем средний возраст по значению фичи\n            mean_age = merged_df.groupby(feature)['age'].mean()\n            # Считаем вероятность 'male' по значению фичи\n            prob_male = merged_df.groupby(feature)['sex'].apply(lambda x: (x == 'male').mean())\n            \n            # Сохраняем результаты\n            encoding['mean_age'] = mean_age\n            encoding['prob_male'] = prob_male\n            \n            self.feature_encoding[feature] = encoding\n\n    def transform(self, feature_df):\n        # Копируем DataFrame, чтобы не изменять оригинал\n        transformed_df = feature_df.copy()\n        \n        features_result = []\n\n        # Для каждой фичи из списка, добавляем новые столбцы с кодировками\n        for feature, encoding in self.feature_encoding.items():\n            # Создаем новые столбцы с кодировками\n            transformed_df[f'{feature}_mean_age'] = transformed_df[feature].map(encoding['mean_age'])\n            transformed_df[f'{feature}_prob_male'] = transformed_df[feature].map(encoding['prob_male'])\n            features_result += [f'{feature}_mean_age', f'{feature}_prob_male']\n        \n        return transformed_df, features_result\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:53.615273Z","iopub.execute_input":"2024-09-29T06:28:53.615617Z","iopub.status.idle":"2024-09-29T06:28:53.633126Z","shell.execute_reply.started":"2024-09-29T06:28:53.615583Z","shell.execute_reply":"2024-09-29T06:28:53.632046Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"def transform_each_cat(data2, feature_name, feature_values, aggregation_method, agg_feature):\n    # Проверяем, есть ли значения для обработки\n    if len(feature_values) == 0:\n        return pd.DataFrame()\n        \n    result = pd.DataFrame()\n\n    # Считаем общее количество событий (total) один раз для всех категорий\n    total_events_per_user = data2.groupby('viewer_uid').size().reset_index(name='total_events')\n    \n    # Для каждого значения фичи создаем новый столбец\n    for value in tqdm(feature_values + ['another_value'], desc=f\"Processing {feature_name} values\"):\n        # Создаем временный столбец, где 1, если категория совпадает, иначе 0\n        col_name = f\"{feature_name}_{value}\"\n        if aggregation_method == 'sum':\n            col_name += '_sum'\n            \n        data2[col_name] = (data2[feature_name] == value).astype(int)\n\n        # Агрегируем по viewer_uid в зависимости от метода агрегации\n        if aggregation_method == 'count':\n            # Считаем количество событий для каждого viewer_uid\n            agg_result = data2.groupby('viewer_uid')[col_name].sum().reset_index()\n            # Считаем долю от общего количества событий\n            agg_result[col_name] = agg_result[col_name] / total_events_per_user['total_events']\n\n        elif aggregation_method == 'sum':\n            # Считаем сумму agg_feature для текущего значения\n            sum_result = data2[data2[feature_name] == value].groupby('viewer_uid')[agg_feature].sum().reset_index()\n            # Считаем сумму agg_feature для всех значений feature_name\n            total_sum = data2.groupby('viewer_uid')[agg_feature].sum().reset_index()\n\n            # Объединяем результаты\n            agg_result = sum_result.merge(total_sum, on='viewer_uid', suffixes=('', '_total'))\n            # Делим на сумму agg_feature для всех значений\n            agg_result[col_name] = agg_result[agg_feature] / agg_result[agg_feature + '_total']\n\n            # Оставляем только нужные столбцы\n            agg_result = agg_result[['viewer_uid', col_name]]\n\n        else:\n            raise ValueError(f\"Unknown aggregation method: {aggregation_method}\")\n\n        # Объединяем результат с основным DataFrame по viewer_uid\n        if result.empty:\n            result = agg_result\n        else:\n            result = result.merge(agg_result, on='viewer_uid', how='outer')\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:53.636873Z","iopub.execute_input":"2024-09-29T06:28:53.637258Z","iopub.status.idle":"2024-09-29T06:28:53.650321Z","shell.execute_reply.started":"2024-09-29T06:28:53.637205Z","shell.execute_reply":"2024-09-29T06:28:53.649342Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"config = {\n    'top_config':\n        {\n        \"region\" : 50,\n        \"ua_device_type\" : 2,\n        \"ua_client_type\" : 2,\n        \"ua_os\" : 6,\n        \"ua_client_name\" : 10,\n        \"category\" : 50,\n        \"author_id\" : 1000,\n        'rutube_video_id': 1000,\n        'view_hour' : 24\n        },\n    'cat_enc':\n    [\n        [\"region\", 'count', 'total_watchtime'],\n        [\"ua_device_type\", 'count', 'total_watchtime'],\n        [\"ua_client_type\", 'count', 'total_watchtime'],\n        [\"ua_os\", 'count', 'total_watchtime'],\n        [\"ua_client_name\", 'count', 'total_watchtime'],\n        [\"category\", 'sum', 'total_watchtime'],\n        [\"category\", 'count', 'total_watchtime'],\n        [\"view_hour\", 'count', 'total_watchtime'],\n        [\"view_hour\", 'sum', 'total_watchtime']\n    ],\n    'target_enc':\n    [\n        'rutube_video_id',\n        'author_id',\n        \"region\",\n        \"ua_device_type\",\n        \"ua_client_type\",\n        \"ua_os\",\n        \"ua_client_name\",\n        \"category\"\n    ]\n}\n\nclass Encoder:\n    def __init__(self, video, config):\n        self.config = config\n        self.video = video\n        self.cat_values = dict()\n        self.target_encoder = TargetEncoder(config)\n        self.category_reducer = CategoryReducer(config)\n        self.target_features = None\n    \n    \n    def pred_transform(self, data):\n        merged = data.merge(self.video, on=(\"rutube_video_id\"))\n        \n        merged = calculate_real_timestamp(merged)\n        merged['share_view'] = 1000 * merged['total_watchtime'] / merged['duration']\n        # Извлекаем час просмотра (значения от 0 до 23)\n        merged['view_hour'] = merged['real_timestamp'].dt.hour\n\n        # Извлекаем время от начала эпохи (Unix-время) в секундах\n        merged['seconds_from_epoch'] = merged['real_timestamp'].apply(lambda x: x.timestamp())\n\n        return merged\n    \n    def fit1(self, merged):\n        self.merged = merged\n        self.category_reducer.fit(merged)\n        self.cat_values = self.category_reducer.cat_values\n    \n    def transform_1_to_1(self, merged):\n        merged = self.category_reducer.transform(merged)\n        \n        return merged\n    \n    def fit2(self, data2, targets):\n        self.target_encoder.fit(data2, targets)\n            \n    \n    def _transform2(self, data2):\n        result, target_features = self.target_encoder.transform(data2)\n        self.target_features = target_features\n        return result\n    \n    def transform_many_to_1(self, data2):\n        # Обработка категориальных фичей\n        # обработка cat_enc\n        def update_result(result, new_result):\n            if result.empty:\n                result = new_result\n            else:\n                result = result.merge(new_result, on='viewer_uid', how='outer')\n            return result\n        \n        result = pd.DataFrame()\n        \n        cat_enc = self.config['cat_enc']\n        for feature_cat_enc in cat_enc:\n            feature_name = feature_cat_enc[0]\n            cat_values_feature = self.cat_values[feature_name]\n            agg_method = feature_cat_enc[1]\n            agg_feature = feature_cat_enc[2]\n            new_result = transform_each_cat(data2.copy(), feature_name,cat_values_feature, agg_method, agg_feature)\n            result = update_result(result, new_result)\n        \n        # обработка числовых фичей\n        numeric_features = data2.select_dtypes(include=['number']).columns.tolist()\n        for feature in ['viewer_uid', 'author_id']:\n            if feature in numeric_features:\n                numeric_features.remove(feature)\n        \n        for feature in numeric_features:   \n            grouped_df = data2.groupby('viewer_uid').agg(\n                {feature: ['mean', 'max', 'min','std', 'sum'],})\n\n            # Сбрасываем индекс и приводим к нормальному виду\n            grouped_df.columns = ['_'.join(col).strip() for col in grouped_df.columns.values]\n            new_result = grouped_df.reset_index()\n            result = update_result(result, new_result)\n        \n        return result","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:53.651632Z","iopub.execute_input":"2024-09-29T06:28:53.651970Z","iopub.status.idle":"2024-09-29T06:28:53.672567Z","shell.execute_reply.started":"2024-09-29T06:28:53.651936Z","shell.execute_reply":"2024-09-29T06:28:53.671511Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"data_test = pd.read_csv('/kaggle/input/rutube-test-dataset/test_events.csv')\n\nTRAIN_IDS = data['viewer_uid'].unique()\nVAL_IDS = data_test['viewer_uid'].unique()\n\ntrain_events = data[data['viewer_uid'].isin(TRAIN_IDS)]\ntrain_targets = targets[targets['viewer_uid'].isin(TRAIN_IDS)]\n\nval_events = data_test","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:53.673812Z","iopub.execute_input":"2024-09-29T06:28:53.674219Z","iopub.status.idle":"2024-09-29T06:28:55.149433Z","shell.execute_reply.started":"2024-09-29T06:28:53.674174Z","shell.execute_reply":"2024-09-29T06:28:55.148549Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"video_emb = reduce_embeddings_with_pca(video_m, 16)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:55.150704Z","iopub.execute_input":"2024-09-29T06:28:55.151092Z","iopub.status.idle":"2024-09-29T06:28:57.806039Z","shell.execute_reply.started":"2024-09-29T06:28:55.151048Z","shell.execute_reply":"2024-09-29T06:28:57.805197Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"video_emb = video_emb.merge(video, on='rutube_video_id')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:57.807168Z","iopub.execute_input":"2024-09-29T06:28:57.807466Z","iopub.status.idle":"2024-09-29T06:28:58.074583Z","shell.execute_reply.started":"2024-09-29T06:28:57.807428Z","shell.execute_reply":"2024-09-29T06:28:58.073701Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"enc = Encoder(video_emb, config)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:58.075685Z","iopub.execute_input":"2024-09-29T06:28:58.075965Z","iopub.status.idle":"2024-09-29T06:28:58.080395Z","shell.execute_reply.started":"2024-09-29T06:28:58.075934Z","shell.execute_reply":"2024-09-29T06:28:58.079339Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"train_events = enc.pred_transform(train_events)\nenc.fit1(train_events)\ntrain_events = enc.transform_1_to_1(train_events)\nenc.fit2(train_events, targets)\ntrain_events = enc._transform2(train_events)\ntrain_events = enc.transform_many_to_1(train_events)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:28:58.081743Z","iopub.execute_input":"2024-09-29T06:28:58.082317Z","iopub.status.idle":"2024-09-29T06:32:07.840043Z","shell.execute_reply.started":"2024-09-29T06:28:58.082281Z","shell.execute_reply":"2024-09-29T06:32:07.839213Z"},"trusted":true},"execution_count":77,"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing region values:   0%|          | 0/51 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c56ce0482d94993b41b3d39e0577d64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing ua_device_type values:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03ae965f276747519840642c03216fb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing ua_client_type values:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ff6646373694d3e8634499c4f306e21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing ua_os values:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6951943d722346c9ad040fe5af2f8af8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing ua_client_name values:   0%|          | 0/11 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd86dcee46ec49f0854c2546b2294992"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing category values:   0%|          | 0/41 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"510cf82e425649c5a97388e4f77b078b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing category values:   0%|          | 0/41 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d6555b1cc1145dfa6e03e9b9963bb95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing view_hour values:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d9976f2aefe4fae9d833eeefded1478"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing view_hour values:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dea1a4a5a3d14b749285957149bb25ee"}},"metadata":{}}]},{"cell_type":"code","source":"val_events = enc.pred_transform(val_events)\nval_events = enc.transform_1_to_1(val_events)\nval_events = enc._transform2(val_events)\nval_events = enc.transform_many_to_1(val_events)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:32:07.841761Z","iopub.execute_input":"2024-09-29T06:32:07.842379Z","iopub.status.idle":"2024-09-29T06:33:04.103496Z","shell.execute_reply.started":"2024-09-29T06:32:07.842330Z","shell.execute_reply":"2024-09-29T06:33:04.102390Z"},"trusted":true},"execution_count":78,"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing region values:   0%|          | 0/51 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fb974df8fe246c89afd9ecc097c278d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing ua_device_type values:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06b811656dc84ecd9ed922672edd0ce4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing ua_client_type values:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c3eaf8029ff442f853c7414f7931a44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing ua_os values:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f0fea4cbc854668bdf641d717a31a82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing ua_client_name values:   0%|          | 0/11 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32b02844b95f4f0f8b5df1f6331f5855"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing category values:   0%|          | 0/41 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"655c478542c34f6fbf16e4f50215b03c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing category values:   0%|          | 0/41 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21a2798f516d4860b4fd3b816d85a2dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing view_hour values:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e0044c4bb0341f2b0928fdb5f3e14bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing view_hour values:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e37d609cc1d470da171809b0f27048a"}},"metadata":{}}]},{"cell_type":"code","source":"def get_data(events, target=None):\n    if target is not None:\n        df_merged = events.merge(target[['viewer_uid', 'age', 'age_class', 'sex']], on='viewer_uid')\n        X = df_merged.drop(columns=['viewer_uid', 'age', 'age_class', 'sex'])  # Убираем viewer_uid и целевую переменную\n        y = df_merged[['age', 'age_class', 'sex']]  # Целевая переменная 'sex'\n        return X, y\n    \n    return events.drop(columns=['viewer_uid']), None","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:33:04.105244Z","iopub.execute_input":"2024-09-29T06:33:04.105580Z","iopub.status.idle":"2024-09-29T06:33:04.111537Z","shell.execute_reply.started":"2024-09-29T06:33:04.105546Z","shell.execute_reply":"2024-09-29T06:33:04.110556Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclass GigaClass:\n    def __init__(self, random_state=42):\n        self.random_state = random_state\n    \n    def load_data(self, X_train, y_train, X_val, y_val):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_val = X_val\n        self.y_val = y_val\n    \n    def train(self, **params):\n        self.model_sex = RandomForestClassifier(**params, random_state=self.random_state)\n        self.model_age = RandomForestClassifier(**params, random_state=self.random_state)\n        \n        self.model_sex.fit(self.X_train.fillna(-1.0), self.y_train['sex'])\n        print('----------------------------------')\n        self.model_age.fit(self.X_train.fillna(-1.0), self.y_train['age'])\n    \n    def plot_sex_importances(self):\n        feature_importances = self.model_sex.get_feature_importance()\n\n        # Создаем DataFrame для удобства отображения\n        feature_names = X_train.columns  # Имена признаков из вашего DataFrame\n        feature_importance_df = pd.DataFrame({\n            'Feature': feature_names,\n            'Importance': feature_importances\n        })\n\n        # Сортируем по значимости\n        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\n        # Визуализация значимости признаков\n        plt.figure(figsize=(10, 8))\n        sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(40))  # Отображаем топ-20 признаков\n        plt.title('Top 20 Most Important Features')\n        plt.xlabel('Importance')\n        plt.ylabel('Features')\n        plt.tight_layout()\n        plt.show()\n    \n    def plot_age_importances(self):\n        feature_importances = self.model_age.get_feature_importance()\n\n        # Создаем DataFrame для удобства отображения\n        feature_names = X_train.columns  # Имена признаков из вашего DataFrame\n        feature_importance_df = pd.DataFrame({\n            'Feature': feature_names,\n            'Importance': feature_importances\n        })\n\n        # Сортируем по значимости\n        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\n        # Визуализация значимости признаков\n        plt.figure(figsize=(10, 8))\n        sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(40))  # Отображаем топ-20 признаков\n        plt.title('Top 20 Most Important Features')\n        plt.xlabel('Importance')\n        plt.ylabel('Features')\n        plt.tight_layout()\n        plt.show()\n        \n    def test(self, X_test=None, y_test=None):\n        if X_test is None:\n            X_test = self.X_val\n        if y_test is None:\n            y_test = self.y_val\n        \n        sex_predicts = self.model_sex.predict(X_test)\n        age_predicts = self.model_age.predict(X_test)\n        \n        age_class_bins = [9, 20, 30, 40, 60] # Возрастные категории пользователей, подробнее в файле с описанием данных\n\n        age_pred = pd.DataFrame()\n        age_pred['age_class'] = pd.cut(age_predicts, bins=age_class_bins, labels=[0, 1, 2, 3]) \n        \n        acc = accuracy_score(sex_predicts, y_test['sex'])\n        f1 = f1_score(y_test['age_class'], age_pred['age_class'], average='weighted')\n        \n        total = 0.3 * acc + 0.7 * f1\n        \n        print(f'Accuracy: {acc} \\n F1: {f1} \\n Total: {total}')\n        \n        return acc, f1, total\n    \n    def predict(self, X_test=None):\n        if X_test is None:\n            X_test = self.X_val\n        sex_predicts = self.model_sex.predict(X_test.fillna(-1.0))\n        age_predicts = self.model_age.predict(X_test.fillna(-1.0))\n        \n        age_class_bins = [9, 20, 30, 40, 60] # Возрастные категории пользователей, подробнее в файле с описанием данных\n\n        age_pred = pd.DataFrame()\n        age_pred['age_class'] = pd.cut(age_predicts, bins=age_class_bins, labels=[0, 1, 2, 3]) \n        \n        submission = pd.DataFrame(columns=['viewer_uid', 'sex', 'age_class'])\n        submission['viewer_uid'] = val_events['viewer_uid']\n        submission['sex'] = sex_predicts\n        submission['age_class'] = age_pred['age_class']\n        \n        return submission","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:33:04.113008Z","iopub.execute_input":"2024-09-29T06:33:04.113322Z","iopub.status.idle":"2024-09-29T06:33:04.135779Z","shell.execute_reply.started":"2024-09-29T06:33:04.113289Z","shell.execute_reply":"2024-09-29T06:33:04.134976Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"train_events_prepared = train_events\nval_events_prepared = val_events","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:33:04.137210Z","iopub.execute_input":"2024-09-29T06:33:04.138074Z","iopub.status.idle":"2024-09-29T06:33:04.151526Z","shell.execute_reply.started":"2024-09-29T06:33:04.138020Z","shell.execute_reply":"2024-09-29T06:33:04.150688Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = get_data(train_events_prepared, train_targets)\nX_test, y_test = get_data(val_events_prepared, None)\n\nmodel = GigaClass()\n\nmodel.load_data(X_train, y_train, X_test, y_test)\nmodel.train(n_estimators=300, max_depth=20, verbose=2, n_jobs=-1) # 800\nsubmission = model.predict()","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:33:04.152630Z","iopub.execute_input":"2024-09-29T06:33:04.152954Z","iopub.status.idle":"2024-09-29T06:41:39.379620Z","shell.execute_reply.started":"2024-09-29T06:33:04.152900Z","shell.execute_reply":"2024-09-29T06:41:39.378576Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n","output_type":"stream"},{"name":"stdout","text":"building tree 1 of 300\nbuilding tree 2 of 300\nbuilding tree 3 of 300\nbuilding tree 4 of 300\nbuilding tree 5 of 300\nbuilding tree 6 of 300\nbuilding tree 7 of 300\nbuilding tree 8 of 300\nbuilding tree 9 of 300\nbuilding tree 10 of 300\nbuilding tree 11 of 300\nbuilding tree 12 of 300\nbuilding tree 13 of 300\nbuilding tree 14 of 300\nbuilding tree 15 of 300\nbuilding tree 16 of 300\nbuilding tree 17 of 300\nbuilding tree 18 of 300\nbuilding tree 19 of 300\nbuilding tree 20 of 300\nbuilding tree 21 of 300\nbuilding tree 22 of 300\nbuilding tree 23 of 300\nbuilding tree 24 of 300\nbuilding tree 25 of 300\nbuilding tree 26 of 300\nbuilding tree 27 of 300\nbuilding tree 28 of 300\nbuilding tree 29 of 300\nbuilding tree 30 of 300\nbuilding tree 31 of 300\nbuilding tree 32 of 300\nbuilding tree 33 of 300\nbuilding tree 34 of 300\nbuilding tree 35 of 300\nbuilding tree 36 of 300\nbuilding tree 37 of 300\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   25.3s\n","output_type":"stream"},{"name":"stdout","text":"building tree 38 of 300\nbuilding tree 39 of 300\nbuilding tree 40 of 300\nbuilding tree 41 of 300\nbuilding tree 42 of 300\nbuilding tree 43 of 300\nbuilding tree 44 of 300\nbuilding tree 45 of 300\nbuilding tree 46 of 300\nbuilding tree 47 of 300\nbuilding tree 48 of 300\nbuilding tree 49 of 300\nbuilding tree 50 of 300\nbuilding tree 51 of 300\nbuilding tree 52 of 300\nbuilding tree 53 of 300\nbuilding tree 54 of 300\nbuilding tree 55 of 300\nbuilding tree 56 of 300\nbuilding tree 57 of 300\nbuilding tree 58 of 300\nbuilding tree 59 of 300\nbuilding tree 60 of 300\nbuilding tree 61 of 300\nbuilding tree 62 of 300\nbuilding tree 63 of 300\nbuilding tree 64 of 300\nbuilding tree 65 of 300\nbuilding tree 66 of 300\nbuilding tree 67 of 300\nbuilding tree 68 of 300\nbuilding tree 69 of 300\nbuilding tree 70 of 300\nbuilding tree 71 of 300\nbuilding tree 72 of 300\nbuilding tree 73 of 300\nbuilding tree 74 of 300\nbuilding tree 75 of 300\nbuilding tree 76 of 300\nbuilding tree 77 of 300\nbuilding tree 78 of 300\nbuilding tree 79 of 300\nbuilding tree 80 of 300\nbuilding tree 81 of 300\nbuilding tree 82 of 300\nbuilding tree 83 of 300\nbuilding tree 84 of 300\nbuilding tree 85 of 300\nbuilding tree 86 of 300\nbuilding tree 87 of 300\nbuilding tree 88 of 300\nbuilding tree 89 of 300\nbuilding tree 90 of 300\nbuilding tree 91 of 300\nbuilding tree 92 of 300\nbuilding tree 93 of 300\nbuilding tree 94 of 300\nbuilding tree 95 of 300\nbuilding tree 96 of 300\nbuilding tree 97 of 300\nbuilding tree 98 of 300\nbuilding tree 99 of 300\nbuilding tree 100 of 300\nbuilding tree 101 of 300\nbuilding tree 102 of 300\nbuilding tree 103 of 300\nbuilding tree 104 of 300\nbuilding tree 105 of 300\nbuilding tree 106 of 300\nbuilding tree 107 of 300\nbuilding tree 108 of 300\nbuilding tree 109 of 300\nbuilding tree 110 of 300\nbuilding tree 111 of 300\nbuilding tree 112 of 300\nbuilding tree 113 of 300\nbuilding tree 114 of 300\nbuilding tree 115 of 300\nbuilding tree 116 of 300\nbuilding tree 117 of 300\nbuilding tree 118 of 300\nbuilding tree 119 of 300\nbuilding tree 120 of 300\nbuilding tree 121 of 300\nbuilding tree 122 of 300\nbuilding tree 123 of 300\nbuilding tree 124 of 300\nbuilding tree 125 of 300\nbuilding tree 126 of 300\nbuilding tree 127 of 300\nbuilding tree 128 of 300\nbuilding tree 129 of 300\nbuilding tree 130 of 300\nbuilding tree 131 of 300\nbuilding tree 132 of 300\nbuilding tree 133 of 300\nbuilding tree 134 of 300\nbuilding tree 135 of 300\nbuilding tree 136 of 300\nbuilding tree 137 of 300\nbuilding tree 138 of 300building tree 139 of 300\n\nbuilding tree 140 of 300\nbuilding tree 141 of 300\nbuilding tree 142 of 300\nbuilding tree 143 of 300\nbuilding tree 144 of 300\nbuilding tree 145 of 300\nbuilding tree 146 of 300\nbuilding tree 147 of 300\nbuilding tree 148 of 300\nbuilding tree 149 of 300\nbuilding tree 150 of 300\nbuilding tree 151 of 300\nbuilding tree 152 of 300\nbuilding tree 153 of 300\nbuilding tree 154 of 300\nbuilding tree 155 of 300\nbuilding tree 156 of 300\nbuilding tree 157 of 300\nbuilding tree 158 of 300\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.9min\n","output_type":"stream"},{"name":"stdout","text":"building tree 159 of 300\nbuilding tree 160 of 300\nbuilding tree 161 of 300\nbuilding tree 162 of 300\nbuilding tree 163 of 300\nbuilding tree 164 of 300\nbuilding tree 165 of 300\nbuilding tree 166 of 300\nbuilding tree 167 of 300\nbuilding tree 168 of 300\nbuilding tree 169 of 300\nbuilding tree 170 of 300\nbuilding tree 171 of 300\nbuilding tree 172 of 300\nbuilding tree 173 of 300\nbuilding tree 174 of 300\nbuilding tree 175 of 300\nbuilding tree 176 of 300\nbuilding tree 177 of 300\nbuilding tree 178 of 300\nbuilding tree 179 of 300\nbuilding tree 180 of 300\nbuilding tree 181 of 300\nbuilding tree 182 of 300\nbuilding tree 183 of 300\nbuilding tree 184 of 300\nbuilding tree 185 of 300\nbuilding tree 186 of 300\nbuilding tree 187 of 300\nbuilding tree 188 of 300\nbuilding tree 189 of 300\nbuilding tree 190 of 300\nbuilding tree 191 of 300\nbuilding tree 192 of 300\nbuilding tree 193 of 300\nbuilding tree 194 of 300\nbuilding tree 195 of 300\nbuilding tree 196 of 300\nbuilding tree 197 of 300\nbuilding tree 198 of 300\nbuilding tree 199 of 300\nbuilding tree 200 of 300\nbuilding tree 201 of 300\nbuilding tree 202 of 300\nbuilding tree 203 of 300\nbuilding tree 204 of 300\nbuilding tree 205 of 300\nbuilding tree 206 of 300\nbuilding tree 207 of 300\nbuilding tree 208 of 300\nbuilding tree 209 of 300\nbuilding tree 210 of 300\nbuilding tree 211 of 300\nbuilding tree 212 of 300\nbuilding tree 213 of 300\nbuilding tree 214 of 300\nbuilding tree 215 of 300\nbuilding tree 216 of 300\nbuilding tree 217 of 300\nbuilding tree 218 of 300\nbuilding tree 219 of 300\nbuilding tree 220 of 300\nbuilding tree 221 of 300\nbuilding tree 222 of 300\nbuilding tree 223 of 300\nbuilding tree 224 of 300\nbuilding tree 225 of 300\nbuilding tree 226 of 300\nbuilding tree 227 of 300\nbuilding tree 228 of 300\nbuilding tree 229 of 300\nbuilding tree 230 of 300\nbuilding tree 231 of 300\nbuilding tree 232 of 300\nbuilding tree 233 of 300\nbuilding tree 234 of 300\nbuilding tree 235 of 300\nbuilding tree 236 of 300\nbuilding tree 237 of 300\nbuilding tree 238 of 300\nbuilding tree 239 of 300\nbuilding tree 240 of 300\nbuilding tree 241 of 300\nbuilding tree 242 of 300\nbuilding tree 243 of 300\nbuilding tree 244 of 300\nbuilding tree 245 of 300\nbuilding tree 246 of 300\nbuilding tree 247 of 300\nbuilding tree 248 of 300\nbuilding tree 249 of 300\nbuilding tree 250 of 300\nbuilding tree 251 of 300\nbuilding tree 252 of 300\nbuilding tree 253 of 300\nbuilding tree 254 of 300\nbuilding tree 255 of 300\nbuilding tree 256 of 300\nbuilding tree 257 of 300\nbuilding tree 258 of 300\nbuilding tree 259 of 300\nbuilding tree 260 of 300\nbuilding tree 261 of 300\nbuilding tree 262 of 300\nbuilding tree 263 of 300\nbuilding tree 264 of 300\nbuilding tree 265 of 300\nbuilding tree 266 of 300\nbuilding tree 267 of 300\nbuilding tree 268 of 300\nbuilding tree 269 of 300\nbuilding tree 270 of 300\nbuilding tree 271 of 300\nbuilding tree 272 of 300\nbuilding tree 273 of 300\nbuilding tree 274 of 300\nbuilding tree 275 of 300\nbuilding tree 276 of 300\nbuilding tree 277 of 300\nbuilding tree 278 of 300\nbuilding tree 279 of 300\nbuilding tree 280 of 300\nbuilding tree 281 of 300\nbuilding tree 282 of 300\nbuilding tree 283 of 300\nbuilding tree 284 of 300\nbuilding tree 285 of 300\nbuilding tree 286 of 300\nbuilding tree 287 of 300\nbuilding tree 288 of 300\nbuilding tree 289 of 300\nbuilding tree 290 of 300\nbuilding tree 291 of 300\nbuilding tree 292 of 300\nbuilding tree 293 of 300\nbuilding tree 294 of 300\nbuilding tree 295 of 300\nbuilding tree 296 of 300\nbuilding tree 297 of 300\nbuilding tree 298 of 300\nbuilding tree 299 of 300\nbuilding tree 300 of 300\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  3.6min finished\n","output_type":"stream"},{"name":"stdout","text":"----------------------------------\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n","output_type":"stream"},{"name":"stdout","text":"building tree 1 of 300\nbuilding tree 2 of 300\nbuilding tree 3 of 300\nbuilding tree 4 of 300\nbuilding tree 5 of 300\nbuilding tree 6 of 300\nbuilding tree 7 of 300\nbuilding tree 8 of 300\nbuilding tree 9 of 300\nbuilding tree 10 of 300\nbuilding tree 11 of 300\nbuilding tree 12 of 300\nbuilding tree 13 of 300\nbuilding tree 14 of 300\nbuilding tree 15 of 300\nbuilding tree 16 of 300\nbuilding tree 17 of 300\nbuilding tree 18 of 300\nbuilding tree 19 of 300\nbuilding tree 20 of 300\nbuilding tree 21 of 300\nbuilding tree 22 of 300\nbuilding tree 23 of 300\nbuilding tree 24 of 300\nbuilding tree 25 of 300\nbuilding tree 26 of 300\nbuilding tree 27 of 300\nbuilding tree 28 of 300\nbuilding tree 29 of 300\nbuilding tree 30 of 300\nbuilding tree 31 of 300\nbuilding tree 32 of 300\nbuilding tree 33 of 300\nbuilding tree 34 of 300\nbuilding tree 35 of 300\nbuilding tree 36 of 300\nbuilding tree 37 of 300\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   35.5s\n","output_type":"stream"},{"name":"stdout","text":"building tree 38 of 300\nbuilding tree 39 of 300\nbuilding tree 40 of 300\nbuilding tree 41 of 300\nbuilding tree 42 of 300\nbuilding tree 43 of 300\nbuilding tree 44 of 300\nbuilding tree 45 of 300\nbuilding tree 46 of 300\nbuilding tree 47 of 300\nbuilding tree 48 of 300\nbuilding tree 49 of 300\nbuilding tree 50 of 300\nbuilding tree 51 of 300\nbuilding tree 52 of 300\nbuilding tree 53 of 300\nbuilding tree 54 of 300\nbuilding tree 55 of 300\nbuilding tree 56 of 300\nbuilding tree 57 of 300\nbuilding tree 58 of 300\nbuilding tree 59 of 300\nbuilding tree 60 of 300\nbuilding tree 61 of 300\nbuilding tree 62 of 300\nbuilding tree 63 of 300\nbuilding tree 64 of 300\nbuilding tree 65 of 300\nbuilding tree 66 of 300\nbuilding tree 67 of 300\nbuilding tree 68 of 300\nbuilding tree 69 of 300\nbuilding tree 70 of 300\nbuilding tree 71 of 300\nbuilding tree 72 of 300\nbuilding tree 73 of 300\nbuilding tree 74 of 300\nbuilding tree 75 of 300\nbuilding tree 76 of 300\nbuilding tree 77 of 300\nbuilding tree 78 of 300\nbuilding tree 79 of 300\nbuilding tree 80 of 300\nbuilding tree 81 of 300\nbuilding tree 82 of 300\nbuilding tree 83 of 300\nbuilding tree 84 of 300\nbuilding tree 85 of 300\nbuilding tree 86 of 300\nbuilding tree 87 of 300\nbuilding tree 88 of 300\nbuilding tree 89 of 300\nbuilding tree 90 of 300\nbuilding tree 91 of 300\nbuilding tree 92 of 300\nbuilding tree 93 of 300\nbuilding tree 94 of 300\nbuilding tree 95 of 300\nbuilding tree 96 of 300\nbuilding tree 97 of 300\nbuilding tree 98 of 300\nbuilding tree 99 of 300\nbuilding tree 100 of 300\nbuilding tree 101 of 300\nbuilding tree 102 of 300\nbuilding tree 103 of 300\nbuilding tree 104 of 300\nbuilding tree 105 of 300\nbuilding tree 106 of 300\nbuilding tree 107 of 300\nbuilding tree 108 of 300\nbuilding tree 109 of 300\nbuilding tree 110 of 300\nbuilding tree 111 of 300\nbuilding tree 112 of 300\nbuilding tree 113 of 300\nbuilding tree 114 of 300\nbuilding tree 115 of 300\nbuilding tree 116 of 300\nbuilding tree 117 of 300\nbuilding tree 118 of 300\nbuilding tree 119 of 300\nbuilding tree 120 of 300\nbuilding tree 121 of 300\nbuilding tree 122 of 300\nbuilding tree 123 of 300\nbuilding tree 124 of 300\nbuilding tree 125 of 300\nbuilding tree 126 of 300\nbuilding tree 127 of 300\nbuilding tree 128 of 300\nbuilding tree 129 of 300\nbuilding tree 130 of 300\nbuilding tree 131 of 300\nbuilding tree 132 of 300\nbuilding tree 133 of 300\nbuilding tree 134 of 300\nbuilding tree 135 of 300\nbuilding tree 136 of 300\nbuilding tree 137 of 300\nbuilding tree 138 of 300\nbuilding tree 139 of 300\nbuilding tree 140 of 300\nbuilding tree 141 of 300\nbuilding tree 142 of 300\nbuilding tree 143 of 300\nbuilding tree 144 of 300\nbuilding tree 145 of 300\nbuilding tree 146 of 300\nbuilding tree 147 of 300\nbuilding tree 148 of 300\nbuilding tree 149 of 300\nbuilding tree 150 of 300\nbuilding tree 151 of 300\nbuilding tree 152 of 300\nbuilding tree 153 of 300\nbuilding tree 154 of 300\nbuilding tree 155 of 300\nbuilding tree 156 of 300\nbuilding tree 157 of 300\nbuilding tree 158 of 300\nbuilding tree 159 of 300\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  2.5min\n","output_type":"stream"},{"name":"stdout","text":"building tree 160 of 300\nbuilding tree 161 of 300\nbuilding tree 162 of 300\nbuilding tree 163 of 300\nbuilding tree 164 of 300\nbuilding tree 165 of 300\nbuilding tree 166 of 300\nbuilding tree 167 of 300\nbuilding tree 168 of 300\nbuilding tree 169 of 300\nbuilding tree 170 of 300\nbuilding tree 171 of 300\nbuilding tree 172 of 300\nbuilding tree 173 of 300\nbuilding tree 174 of 300\nbuilding tree 175 of 300\nbuilding tree 176 of 300\nbuilding tree 177 of 300\nbuilding tree 178 of 300\nbuilding tree 179 of 300\nbuilding tree 180 of 300\nbuilding tree 181 of 300\nbuilding tree 182 of 300\nbuilding tree 183 of 300\nbuilding tree 184 of 300\nbuilding tree 185 of 300\nbuilding tree 186 of 300\nbuilding tree 187 of 300\nbuilding tree 188 of 300\nbuilding tree 189 of 300\nbuilding tree 190 of 300\nbuilding tree 191 of 300\nbuilding tree 192 of 300\nbuilding tree 193 of 300\nbuilding tree 194 of 300\nbuilding tree 195 of 300\nbuilding tree 196 of 300\nbuilding tree 197 of 300\nbuilding tree 198 of 300\nbuilding tree 199 of 300\nbuilding tree 200 of 300\nbuilding tree 201 of 300\nbuilding tree 202 of 300\nbuilding tree 203 of 300\nbuilding tree 204 of 300\nbuilding tree 205 of 300\nbuilding tree 206 of 300\nbuilding tree 207 of 300\nbuilding tree 208 of 300\nbuilding tree 209 of 300\nbuilding tree 210 of 300\nbuilding tree 211 of 300\nbuilding tree 212 of 300\nbuilding tree 213 of 300\nbuilding tree 214 of 300\nbuilding tree 215 of 300\nbuilding tree 216 of 300\nbuilding tree 217 of 300\nbuilding tree 218 of 300\nbuilding tree 219 of 300\nbuilding tree 220 of 300\nbuilding tree 221 of 300\nbuilding tree 222 of 300\nbuilding tree 223 of 300\nbuilding tree 224 of 300\nbuilding tree 225 of 300\nbuilding tree 226 of 300\nbuilding tree 227 of 300\nbuilding tree 228 of 300\nbuilding tree 229 of 300\nbuilding tree 230 of 300\nbuilding tree 231 of 300\nbuilding tree 232 of 300\nbuilding tree 233 of 300\nbuilding tree 234 of 300\nbuilding tree 235 of 300\nbuilding tree 236 of 300\nbuilding tree 237 of 300\nbuilding tree 238 of 300\nbuilding tree 239 of 300\nbuilding tree 240 of 300\nbuilding tree 241 of 300\nbuilding tree 242 of 300\nbuilding tree 243 of 300\nbuilding tree 244 of 300\nbuilding tree 245 of 300\nbuilding tree 246 of 300\nbuilding tree 247 of 300\nbuilding tree 248 of 300\nbuilding tree 249 of 300\nbuilding tree 250 of 300\nbuilding tree 251 of 300\nbuilding tree 252 of 300\nbuilding tree 253 of 300\nbuilding tree 254 of 300\nbuilding tree 255 of 300\nbuilding tree 256 of 300\nbuilding tree 257 of 300\nbuilding tree 258 of 300\nbuilding tree 259 of 300\nbuilding tree 260 of 300\nbuilding tree 261 of 300\nbuilding tree 262 of 300\nbuilding tree 263 of 300\nbuilding tree 264 of 300\nbuilding tree 265 of 300\nbuilding tree 266 of 300\nbuilding tree 267 of 300\nbuilding tree 268 of 300\nbuilding tree 269 of 300\nbuilding tree 270 of 300\nbuilding tree 271 of 300\nbuilding tree 272 of 300\nbuilding tree 273 of 300\nbuilding tree 274 of 300\nbuilding tree 275 of 300\nbuilding tree 276 of 300\nbuilding tree 277 of 300\nbuilding tree 278 of 300\nbuilding tree 279 of 300\nbuilding tree 280 of 300\nbuilding tree 281 of 300\nbuilding tree 282 of 300\nbuilding tree 283 of 300\nbuilding tree 284 of 300\nbuilding tree 285 of 300\nbuilding tree 286 of 300\nbuilding tree 287 of 300\nbuilding tree 288 of 300\nbuilding tree 289 of 300\nbuilding tree 290 of 300\nbuilding tree 291 of 300\nbuilding tree 292 of 300\nbuilding tree 293 of 300\nbuilding tree 294 of 300\nbuilding tree 295 of 300\nbuilding tree 296 of 300\nbuilding tree 297 of 300\nbuilding tree 298 of 300\nbuilding tree 299 of 300\nbuilding tree 300 of 300\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  4.8min finished\n[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    0.2s\n[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:    1.1s\n[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    2.2s finished\n[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:    2.1s\n[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    3.8s finished\n","output_type":"stream"}]},{"cell_type":"code","source":"submission.to_csv(\n    \"/kaggle/working/sumb.csv\",\n    index=False \n)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T06:46:36.016888Z","iopub.execute_input":"2024-09-29T06:46:36.017785Z","iopub.status.idle":"2024-09-29T06:46:36.136542Z","shell.execute_reply.started":"2024-09-29T06:46:36.017741Z","shell.execute_reply":"2024-09-29T06:46:36.135643Z"},"trusted":true},"execution_count":95,"outputs":[]}]}